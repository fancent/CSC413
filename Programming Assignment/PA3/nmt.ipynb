{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "1975e2a4-5c73-4262-f208-3642b7609b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /My Drive/University/CSC413/P3/\n",
        "%cd /My Drive/University/CSC413/P3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Collecting pillow>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/5e/23dcc0ce3cc2abe92efd3cd61d764bee6ccdf1b667a1fb566f45dc249953/Pillow-7.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 27.7MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 37.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 24.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 29.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 15.3MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 14.1MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 13.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 13.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 102kB 13.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 13.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 122kB 13.0MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 143kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 153kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 174kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 13.0MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 13.0MB/s eta 0:00:01\r\u001b[K     |███                             | 204kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 215kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 225kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 245kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 256kB 13.0MB/s eta 0:00:01\r\u001b[K     |████                            | 266kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 276kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 286kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 296kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 307kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 317kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 337kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 348kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 358kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 368kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 378kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 399kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 409kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 419kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 430kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 440kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 450kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 460kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 471kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 481kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 491kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 501kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 512kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 532kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 542kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 552kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 563kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 573kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 583kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 593kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 604kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 614kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 624kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 634kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 645kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 655kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 665kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 675kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 686kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 696kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 706kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 716kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 727kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 737kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 747kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 757kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 768kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 778kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 788kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 798kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 808kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 819kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 829kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 839kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 849kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 860kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 870kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 880kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 890kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 901kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 911kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 921kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 931kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 942kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 952kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 962kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 972kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 983kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 993kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.9MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.1)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-7.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/My Drive/University/CSC413/P3'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "      encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                          hidden_size=opts.hidden_size, \n",
        "                          opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "      encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                   hidden_size=opts.hidden_size, \n",
        "                                   num_layers=opts.num_transformer_layers,\n",
        "                                   opts=opts)\n",
        "    else:\n",
        "        print(\"why is it here\")\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        print(\"why is it here\")\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab_type": "code",
        "outputId": "a12adfe1-5abd-47e4-97dc-c880b8ec3b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Gated Recurrent Unit (GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the Gated Recurent Unit class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMO7FD6l5RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) * self.Whh(h_prev))\n",
        "        h_new = (1-z) * g + z * h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None        \n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv25zCMQGVzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab_type": "code",
        "outputId": "340fa1ec-debc-46a4-bb25-f34ffd72590a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('atmosphere', 'atmosphereway')\n",
            "('self-consequence', 'elfsay-onsequencecay')\n",
            "('extinguished', 'extinguishedway')\n",
            "('began', 'eganbay')\n",
            "('change', 'angechay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.347 | Val loss: 2.079 | Gen: onway-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay onway-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay onway-ay-ay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.944 | Val loss: 1.929 | Gen: engay-ayday ayday onway-onay-onay-ay-a onway-ayday onway-onay-onay-ay-a\n",
            "Epoch:   2 | Train loss: 1.796 | Val loss: 1.843 | Gen: eray-ayday ayday ontay-ingay-ayday ayday ontay-ingay-ayday\n",
            "Epoch:   3 | Train loss: 1.704 | Val loss: 1.780 | Gen: eray-ayday ayday onestay-ayday ayday onedway\n",
            "Epoch:   4 | Train loss: 1.637 | Val loss: 1.745 | Gen: eray-ayday away-ayday ontay-anday-anday-ay ingway otedway\n",
            "Epoch:   5 | Train loss: 1.585 | Val loss: 1.705 | Gen: eray away-onay-ayday ontay-ayday ingway otedway\n",
            "Epoch:   6 | Train loss: 1.548 | Val loss: 1.686 | Gen: eway ayday ontay-atedway ingway otedway\n",
            "Epoch:   7 | Train loss: 1.508 | Val loss: 1.656 | Gen: eway away-onay-onay-ayday oonesstay-andway-ayd ilway ongay-ayday\n",
            "Epoch:   8 | Train loss: 1.480 | Val loss: 1.629 | Gen: eway away-ortay-oray-ayda oninglay-onay-onay-a ingay-oray-ayday oterway\n",
            "Epoch:   9 | Train loss: 1.454 | Val loss: 1.635 | Gen: eway away-ortay-onay-onay onay-onay-onay-andwa iway orteray-ayday\n",
            "Epoch:  10 | Train loss: 1.438 | Val loss: 1.607 | Gen: eway away-ingay-ingay-ayd onsesssay-ingay-andw ingay-ayday otertay-ayday\n",
            "Epoch:  11 | Train loss: 1.418 | Val loss: 1.626 | Gen: eway away-ingay-andway-ay onay-ingsay-ingay-ay iway ortay-ingay-ayday\n",
            "Epoch:  12 | Train loss: 1.418 | Val loss: 1.585 | Gen: eway away-ingay-ayday onsay-onay-andway-in ingay-ayday ortay-away-away-away\n",
            "Epoch:  13 | Train loss: 1.380 | Val loss: 1.553 | Gen: eway away-ingay-ingay onationssay-ayday intway ortay-awlay-awlay-aw\n",
            "Epoch:  14 | Train loss: 1.361 | Val loss: 1.549 | Gen: eway away-ingray-ayday onay-andentay-indedw intay ortay-awlay-awlay-ay\n",
            "Epoch:  15 | Train loss: 1.341 | Val loss: 1.531 | Gen: eway away-ingay-ayday onay-ingay-inday-and iway ortay-ayday\n",
            "Epoch:  16 | Train loss: 1.320 | Val loss: 1.505 | Gen: eway away-ingray onsay-indentay-away- indway ortay-awlay-awlay\n",
            "Epoch:  17 | Train loss: 1.309 | Val loss: 1.515 | Gen: eway away-ingray-ayday onsay-inday-ingay-ay intay-ayday ortay-awlay-awlay-ay\n",
            "Epoch:  18 | Train loss: 1.299 | Val loss: 1.494 | Gen: eway ayday onay-ingsay-ayday inday ortay-awlay\n",
            "Epoch:  19 | Train loss: 1.279 | Val loss: 1.482 | Gen: eway away-ingray-ayday onablinglay-indessay intay-awlay ortay-awlay-awlay\n",
            "Epoch:  20 | Train loss: 1.271 | Val loss: 1.461 | Gen: eway airway onsay-indentay-away- indway ortay-away-etay-away\n",
            "Epoch:  21 | Train loss: 1.260 | Val loss: 1.477 | Gen: elyway airway onanionsessay iway ortay-awlay-ayday\n",
            "Epoch:  22 | Train loss: 1.255 | Val loss: 1.482 | Gen: eway aray-away-away-away- onsay-indentay-away- insay ortedway\n",
            "Epoch:  23 | Train loss: 1.239 | Val loss: 1.453 | Gen: eway aray-ingray ontay-indentay-ingra intay ortay-awlay\n",
            "Epoch:  24 | Train loss: 1.215 | Val loss: 1.426 | Gen: eway airay-ayday onsay-indessay iway ortay-away-ayday\n",
            "Epoch:  25 | Train loss: 1.196 | Val loss: 1.421 | Gen: eway aringway onsay-indessay intay ortedway\n",
            "Epoch:  26 | Train loss: 1.186 | Val loss: 1.405 | Gen: eway aringray ontay-indentay-inden iway ortay-indedway\n",
            "Epoch:  27 | Train loss: 1.178 | Val loss: 1.396 | Gen: eway aringray onsingsay-indessay intay ortedway\n",
            "Epoch:  28 | Train loss: 1.174 | Val loss: 1.474 | Gen: eway array-away-etay-ated oningsay-indentay-in intay orray-away-ayday\n",
            "Epoch:  29 | Train loss: 1.203 | Val loss: 1.466 | Gen: eway artay-ingray onday-indentay-awlay iway orway\n",
            "Epoch:  30 | Train loss: 1.181 | Val loss: 1.441 | Gen: eway artay-ingray onsay-ingsay-away-et iway ortay-indedway\n",
            "Epoch:  31 | Train loss: 1.170 | Val loss: 1.395 | Gen: eway artay-ingray-ayday ontay-indingsay-inge iway ortay-indedway\n",
            "Epoch:  32 | Train loss: 1.144 | Val loss: 1.374 | Gen: eway artay-ingray ontay-ingsay-ingsay- intay ortedsay-ayday\n",
            "Epoch:  33 | Train loss: 1.131 | Val loss: 1.380 | Gen: eway artay-indedway ontay-ingsay-ingsay- iway ortedsay\n",
            "Epoch:  34 | Train loss: 1.123 | Val loss: 1.360 | Gen: eway aray-away-eteray-awa ontay-indentay-ingsa intay ortersay-alway\n",
            "Epoch:  35 | Train loss: 1.116 | Val loss: 1.368 | Gen: eway aray-ationway ontaingingsay-indedw intay ortousedway\n",
            "Epoch:  36 | Train loss: 1.110 | Val loss: 1.360 | Gen: eway aray-away-etentay ontay-indablay-ingsa intay ortousedway\n",
            "Epoch:  37 | Train loss: 1.106 | Val loss: 1.377 | Gen: eway arationway ondingsay-indentay-i intay ortay-indedway\n",
            "Epoch:  38 | Train loss: 1.102 | Val loss: 1.389 | Gen: eway arationway onsingsay-indestay-i istay ortelysay-ayday\n",
            "Epoch:  39 | Train loss: 1.144 | Val loss: 1.394 | Gen: eway aringray ontay-indessay-ingra intay ortelysay\n",
            "Epoch:  40 | Train loss: 1.122 | Val loss: 1.399 | Gen: eway arationway ontanionssay-indessa intay ortenday-indedway\n",
            "Epoch:  41 | Train loss: 1.110 | Val loss: 1.377 | Gen: eblay artay-away-etetay onsingsay-indedsay-i istway ortousedway\n",
            "Epoch:  42 | Train loss: 1.101 | Val loss: 1.339 | Gen: eway artay-awlay ontaingsay-indestay istay ortedsay\n",
            "Epoch:  43 | Train loss: 1.076 | Val loss: 1.314 | Gen: eway artay-indedway ontingsay-indestay-i istay ortedsay-ayday\n",
            "Epoch:  44 | Train loss: 1.061 | Val loss: 1.302 | Gen: eway aringray ontingsay-indestainw intay ortousedway\n",
            "Epoch:  45 | Train loss: 1.053 | Val loss: 1.343 | Gen: eway artay-indedway ontingsay-indestay-i istay ortedsay-ayday\n",
            "Epoch:  46 | Train loss: 1.046 | Val loss: 1.302 | Gen: eway artay-indedway ontingsay-indestay-i intay ortousedway\n",
            "Epoch:  47 | Train loss: 1.038 | Val loss: 1.331 | Gen: eway artay-indedway ontanionsay istay ortedsay\n",
            "Epoch:  48 | Train loss: 1.031 | Val loss: 1.315 | Gen: eway artay-inday ontingsay-indedsay-a istay ortousedway\n",
            "Epoch:  49 | Train loss: 1.026 | Val loss: 1.338 | Gen: eway aringay ontanionabledway istay ortonableway\n",
            "Epoch:  50 | Train loss: 1.028 | Val loss: 1.321 | Gen: eway aray-away-atedway ontingsay-indedsay-a istay ortelay\n",
            "Epoch:  51 | Train loss: 1.025 | Val loss: 1.364 | Gen: eway artay-idway ontingsay-ingray imway ortonsay\n",
            "Epoch:  52 | Train loss: 1.037 | Val loss: 1.284 | Gen: eway arinday ontingsay-indemay-in impay ortonsay\n",
            "Epoch:  53 | Train loss: 1.017 | Val loss: 1.316 | Gen: eway aray-andway ontanionablenedway istay ortenday-awlay\n",
            "Epoch:  54 | Train loss: 1.012 | Val loss: 1.331 | Gen: eway artay-inday ontingsay-indertay-a imway ortelyway\n",
            "Epoch:  55 | Train loss: 1.032 | Val loss: 1.289 | Gen: eyway arinday oningsay-indersay-in impay ortonabledway\n",
            "Epoch:  56 | Train loss: 1.010 | Val loss: 1.292 | Gen: eway airway ontingsay-indemay-in impay ortingsay\n",
            "Epoch:  57 | Train loss: 1.001 | Val loss: 1.288 | Gen: ehay aridway ontingsay-indientay- impay ortonableway\n",
            "Epoch:  58 | Train loss: 0.992 | Val loss: 1.261 | Gen: eway arationway ontingsay-indemay-in istay ortenday-owlay\n",
            "Epoch:  59 | Train loss: 0.982 | Val loss: 1.250 | Gen: eway araway-indedway ondingsay-indertay-i imway ortonsay\n",
            "Epoch:  60 | Train loss: 0.981 | Val loss: 1.267 | Gen: eway aridway oningsay-indemanione impay ortonsay-etay-awlay\n",
            "Epoch:  61 | Train loss: 0.981 | Val loss: 1.253 | Gen: eway araway-indedway ondingsay-ortingpay istay ortionsay\n",
            "Epoch:  62 | Train loss: 0.986 | Val loss: 1.232 | Gen: eway aridway ontingsay-indersay-i imway ortonsay\n",
            "Epoch:  63 | Train loss: 0.969 | Val loss: 1.234 | Gen: eway arationway oningsay-indemay-ind impay ortonsray\n",
            "Epoch:  64 | Train loss: 0.964 | Val loss: 1.220 | Gen: eway airway oningsay-ortentancew imway ortonsay\n",
            "Epoch:  65 | Train loss: 0.956 | Val loss: 1.224 | Gen: eway arinday oningsay-indemay-ind imway ortonsay\n",
            "Epoch:  66 | Train loss: 0.957 | Val loss: 1.233 | Gen: eway araway-away-ay-eteta ondingsay-indemay-in istay ortonsay\n",
            "Epoch:  67 | Train loss: 0.961 | Val loss: 1.228 | Gen: eyway airway ontingsay-ortentainc imway ortonsray\n",
            "Epoch:  68 | Train loss: 0.948 | Val loss: 1.227 | Gen: eway airway oningsay-indemay-ind impay ortonsay\n",
            "Epoch:  69 | Train loss: 0.940 | Val loss: 1.213 | Gen: eway airway oningsay-ortingpay imway ortonsay\n",
            "Epoch:  70 | Train loss: 0.939 | Val loss: 1.211 | Gen: eway araway-indedway oningsay-ortingpay-e impay ortonsray\n",
            "Epoch:  71 | Train loss: 0.929 | Val loss: 1.198 | Gen: eway airway oningpay-ompentainca imway ortonsay\n",
            "Epoch:  72 | Train loss: 0.929 | Val loss: 1.220 | Gen: eway araway-anday-awlay oningpay-ompentainca imway ortonsray\n",
            "Epoch:  73 | Train loss: 0.943 | Val loss: 1.228 | Gen: eway airway oningpay-indemay-ind imway ortionay\n",
            "Epoch:  74 | Train loss: 0.945 | Val loss: 1.207 | Gen: eway airway oningpay-ompentancew imsay ortonsay\n",
            "Epoch:  75 | Train loss: 0.919 | Val loss: 1.185 | Gen: eway airway oningpanionedway imway ortonsay\n",
            "Epoch:  76 | Train loss: 0.911 | Val loss: 1.180 | Gen: eway airway oningpay-otentay-ete imway ortonsray\n",
            "Epoch:  77 | Train loss: 0.906 | Val loss: 1.186 | Gen: eway airway oningpay-oonespray imway ortonsray\n",
            "Epoch:  78 | Train loss: 0.906 | Val loss: 1.196 | Gen: eway araddway oningpay-otentay-ett imsay ortonsray\n",
            "Epoch:  79 | Train loss: 0.900 | Val loss: 1.182 | Gen: eway airway oningpanionedway istay ortonsay\n",
            "Epoch:  80 | Train loss: 0.918 | Val loss: 1.237 | Gen: eway arainay oningpay-incentay-et imsay orfay-ingpay\n",
            "Epoch:  81 | Train loss: 0.952 | Val loss: 1.228 | Gen: eyway airway ontinginationcay imsay ortonscay\n",
            "Epoch:  82 | Train loss: 0.917 | Val loss: 1.167 | Gen: eway airway oningingay-otedsay istay orncay-odsedway\n",
            "Epoch:  83 | Train loss: 0.903 | Val loss: 1.187 | Gen: eway airway oningpanionedway istay ortonsay\n",
            "Epoch:  84 | Train loss: 0.890 | Val loss: 1.166 | Gen: eway airway oningpanionedway imway ortonsay\n",
            "Epoch:  85 | Train loss: 0.882 | Val loss: 1.147 | Gen: eway airway oningponday-etentay istay orncay-odshay\n",
            "Epoch:  86 | Train loss: 0.880 | Val loss: 1.161 | Gen: eway araddway-ybay oningpanionedway imway ortolway\n",
            "Epoch:  87 | Train loss: 0.883 | Val loss: 1.179 | Gen: eway airway oningpay-otentaincay istay orncay-owleway\n",
            "Epoch:  88 | Train loss: 0.915 | Val loss: 1.209 | Gen: eway airway oningpanionedway imstway orndlay\n",
            "Epoch:  89 | Train loss: 0.884 | Val loss: 1.156 | Gen: eway airway oningpondedsay istay ornchiysway\n",
            "Epoch:  90 | Train loss: 0.871 | Val loss: 1.147 | Gen: eway airway oningponday-etentay istay orniokedway\n",
            "Epoch:  91 | Train loss: 0.864 | Val loss: 1.146 | Gen: eway airway oningportanceway istay ornchiysay\n",
            "Epoch:  92 | Train loss: 0.860 | Val loss: 1.160 | Gen: eway airway oningponday-etentay istay ornchiysay\n",
            "Epoch:  93 | Train loss: 0.860 | Val loss: 1.156 | Gen: eway airway oningponday istay ornchay-owlay\n",
            "Epoch:  94 | Train loss: 0.855 | Val loss: 1.152 | Gen: eway airway oningponday istay ornchiysay\n",
            "Epoch:  95 | Train loss: 0.852 | Val loss: 1.149 | Gen: eway airway oningponday-etentay istay ornchiysay\n",
            "Epoch:  96 | Train loss: 0.858 | Val loss: 1.172 | Gen: eway airway oningpay-oonemedway imsay ornchiysway\n",
            "Epoch:  97 | Train loss: 0.858 | Val loss: 1.160 | Gen: eway airway oningponday istay ornchiysway\n",
            "Epoch:  98 | Train loss: 0.852 | Val loss: 1.182 | Gen: eway airway oningingpay istay ornchiy-ay-awlyway\n",
            "Epoch:  99 | Train loss: 0.850 | Val loss: 1.171 | Gen: eway airway oningpanionedway istay ornchiysway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\teway airway oningpanionedway istay ornchiysway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt",
        "colab_type": "text"
      },
      "source": [
        "Try translating different sentences by changing the variable TEST_SENTENCE. Identify two distinct failure modes and briefly describe them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab_type": "code",
        "outputId": "d41b7472-9c8c-4814-9717-49f327d6f167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "TEST_SENTENCE = 'tea team tight'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "TEST_SENTENCE = 'shopping fighting running'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\ttea team tight \n",
            "translated:\teatay eaway ightedway\n",
            "source:\t\tshopping fighting running \n",
            "translated:\tosingpay ightfay-indedway uningway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN - START\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch (batch size x hidden size)\n",
        "            context, attention_weights = self.attention(embed_current, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, torch.squeeze(context, dim=1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size      \n",
        "            # ------------\n",
        "            # FILL THIS IN - END\n",
        "            # ------------     \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3-FuzY1pepu",
        "colab_type": "code",
        "outputId": "12f2df4c-6c17-42b2-a61d-542ed4d5ed2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('directly', 'irectlyday')\n",
            "('bless', 'essblay')\n",
            "('compromise', 'ompromisecay')\n",
            "('hesitatingly', 'esitatinglyhay')\n",
            "('irritated', 'irritatedway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.362 | Val loss: 2.091 | Gen: onay away insay-ay onay insay\n",
            "Epoch:   1 | Train loss: 1.932 | Val loss: 1.947 | Gen: ayway away-ayway insay-away-ayway away-ayway away\n",
            "Epoch:   2 | Train loss: 1.785 | Val loss: 1.818 | Gen: otay away ontingway away ollllay\n",
            "Epoch:   3 | Train loss: 1.653 | Val loss: 1.757 | Gen: edway away onglay-antay ay-ay-ay-ay ollay-ay-ay\n",
            "Epoch:   4 | Train loss: 1.546 | Val loss: 1.704 | Gen: eray away ongway isay otinglay\n",
            "Epoch:   5 | Train loss: 1.460 | Val loss: 1.599 | Gen: eray away onglay isway olllay-ingway\n",
            "Epoch:   6 | Train loss: 1.379 | Val loss: 1.553 | Gen: eray away onglay-ingay isway ollway-ingway\n",
            "Epoch:   7 | Train loss: 1.317 | Val loss: 1.462 | Gen: etay awlay onglay-ingay istay oringlway\n",
            "Epoch:   8 | Train loss: 1.216 | Val loss: 1.400 | Gen: etoway away ongingingingay istay oringlingnay\n",
            "Epoch:   9 | Train loss: 1.126 | Val loss: 1.314 | Gen: etoway away-ayday ongfay-ongay-ongay-o istay oringlingway\n",
            "Epoch:  10 | Train loss: 1.053 | Val loss: 1.281 | Gen: etay awlway onginglingay-ingay-i istay orglingnay\n",
            "Epoch:  11 | Train loss: 1.029 | Val loss: 1.292 | Gen: etay airay oningday istay origlway\n",
            "Epoch:  12 | Train loss: 0.961 | Val loss: 1.186 | Gen: etay away onginglingday istay orglway-ingway\n",
            "Epoch:  13 | Train loss: 0.896 | Val loss: 1.167 | Gen: etay away oningdingday istay orglway-incay\n",
            "Epoch:  14 | Train loss: 0.861 | Val loss: 1.133 | Gen: eway airway ongway-ingway istay orglway\n",
            "Epoch:  15 | Train loss: 0.809 | Val loss: 1.092 | Gen: etay away ondingday ispway orglway-ingway\n",
            "Epoch:  16 | Train loss: 0.767 | Val loss: 1.036 | Gen: etay airway oningway ispay orglway\n",
            "Epoch:  17 | Train loss: 0.713 | Val loss: 0.990 | Gen: ethay airway oningingway ispay orglingway\n",
            "Epoch:  18 | Train loss: 0.670 | Val loss: 0.958 | Gen: eay away oningfay ispay orglingway\n",
            "Epoch:  19 | Train loss: 0.628 | Val loss: 0.911 | Gen: eway away onitingway isway orglingway\n",
            "Epoch:  20 | Train loss: 0.604 | Val loss: 0.892 | Gen: eathay away ongfay ispay orglingway\n",
            "Epoch:  21 | Train loss: 0.584 | Val loss: 0.865 | Gen: ethay airway onitinitingway isway orglingway\n",
            "Epoch:  22 | Train loss: 0.567 | Val loss: 0.935 | Gen: ethay airway oningway ispisway orkingway-ay-oryway\n",
            "Epoch:  23 | Train loss: 0.555 | Val loss: 0.818 | Gen: eay airway ondingway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.523 | Val loss: 0.820 | Gen: eay airway oningway isway orkingway\n",
            "Epoch:  25 | Train loss: 0.504 | Val loss: 0.773 | Gen: etay airway ondingway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.475 | Val loss: 0.761 | Gen: eay airway oningway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.456 | Val loss: 0.751 | Gen: ethay airway ondingway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.445 | Val loss: 0.803 | Gen: etay airway ondingway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.435 | Val loss: 0.773 | Gen: eay airway onininininininininin isway orkingway\n",
            "Epoch:  30 | Train loss: 0.414 | Val loss: 0.706 | Gen: ethay airway onindinayfway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.386 | Val loss: 0.679 | Gen: ethay airway ondingway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.374 | Val loss: 0.653 | Gen: ethay airway onindinayfway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.370 | Val loss: 0.698 | Gen: ethay away ongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.371 | Val loss: 0.662 | Gen: ethay airway ondindionday isway orkingway\n",
            "Epoch:  35 | Train loss: 0.350 | Val loss: 0.681 | Gen: ethay airway ondingway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.351 | Val loss: 0.733 | Gen: ethay airway onitionionitionionin isway orkingway\n",
            "Epoch:  37 | Train loss: 0.364 | Val loss: 0.798 | Gen: eay airway onditioncay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.400 | Val loss: 0.678 | Gen: eathay away ondionitionitiondion isway orkingway\n",
            "Epoch:  39 | Train loss: 0.349 | Val loss: 0.603 | Gen: ethay airway ondinditinditinditin isway orkingway\n",
            "Epoch:  40 | Train loss: 0.322 | Val loss: 0.600 | Gen: ethay airway ondionditindiondiond isway orkingway\n",
            "Epoch:  41 | Train loss: 0.313 | Val loss: 0.614 | Gen: ethay airway ondiondiondiondionca isway orkingway\n",
            "Epoch:  42 | Train loss: 0.311 | Val loss: 0.610 | Gen: ethay airway onditionditiondition isway orkingway\n",
            "Epoch:  43 | Train loss: 0.312 | Val loss: 0.644 | Gen: ethay away ondiondiondioncay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.306 | Val loss: 0.576 | Gen: ethay airway ondionditionditionca isway orkingway\n",
            "Epoch:  45 | Train loss: 0.294 | Val loss: 0.601 | Gen: ethay airway ondiondioncay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.285 | Val loss: 0.573 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.276 | Val loss: 0.575 | Gen: ethay airway ondiondioncay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.271 | Val loss: 0.592 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.268 | Val loss: 0.569 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.261 | Val loss: 0.571 | Gen: ethay away ondioncay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.258 | Val loss: 0.593 | Gen: ethay away ondindioncay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.253 | Val loss: 0.596 | Gen: ethay airway ondionionionionitine isway orkingway\n",
            "Epoch:  53 | Train loss: 0.250 | Val loss: 0.582 | Gen: ethay airway ondiondioncay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.254 | Val loss: 0.616 | Gen: ethay away onitionitionionitina isway orkingway\n",
            "Epoch:  55 | Train loss: 0.257 | Val loss: 0.604 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.252 | Val loss: 0.609 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.248 | Val loss: 0.666 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.256 | Val loss: 0.564 | Gen: ethay airway ondioncay-onday isway orkingway\n",
            "Epoch:  59 | Train loss: 0.239 | Val loss: 0.588 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.231 | Val loss: 0.569 | Gen: ethay airway ondionionionitinay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.222 | Val loss: 0.556 | Gen: ethay airway ondioncay-ondioncay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.215 | Val loss: 0.548 | Gen: ethay airway ondioncay-ondioncay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.211 | Val loss: 0.555 | Gen: ethay airway ondioncay-ondioncay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.209 | Val loss: 0.575 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.215 | Val loss: 0.606 | Gen: ethay airway onditindioncay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.248 | Val loss: 0.754 | Gen: ethay airway ondionditioncay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.271 | Val loss: 0.627 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.224 | Val loss: 0.551 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.211 | Val loss: 0.540 | Gen: ethay airway ondioncay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.202 | Val loss: 0.530 | Gen: ethay airway ondioncay-ondioncay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.195 | Val loss: 0.585 | Gen: ethay airway ondionitioncay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.198 | Val loss: 0.524 | Gen: ethay airway ondioncay-ondioncay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.190 | Val loss: 0.543 | Gen: ethay airway ondioncay-onditionca isway orkingway\n",
            "Epoch:  74 | Train loss: 0.183 | Val loss: 0.524 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.180 | Val loss: 0.528 | Gen: ethay airway ondioncay-onditionca isway orkingway\n",
            "Epoch:  76 | Train loss: 0.177 | Val loss: 0.519 | Gen: ethay airway ondioncay-ondioncay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.176 | Val loss: 0.539 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.176 | Val loss: 0.530 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.173 | Val loss: 0.541 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.172 | Val loss: 0.538 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.170 | Val loss: 0.555 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.172 | Val loss: 0.535 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.173 | Val loss: 0.629 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.182 | Val loss: 0.601 | Gen: ethay airway ondititioncay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.180 | Val loss: 0.538 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.169 | Val loss: 0.532 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.163 | Val loss: 0.536 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.159 | Val loss: 0.543 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.158 | Val loss: 0.535 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.154 | Val loss: 0.543 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.154 | Val loss: 0.547 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.156 | Val loss: 0.565 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.160 | Val loss: 0.576 | Gen: ethay airway onditioncay-ondionca isway orkingway\n",
            "Epoch:  94 | Train loss: 0.172 | Val loss: 0.652 | Gen: ethay airway onditinditioncay isway orkinbay-ingway\n",
            "Epoch:  95 | Train loss: 0.176 | Val loss: 0.579 | Gen: ethay airway onditinditioncay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.159 | Val loss: 0.558 | Gen: etthay airway onditioncay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.155 | Val loss: 0.557 | Gen: ethay airway onditioncaycycay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.155 | Val loss: 0.529 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.160 | Val loss: 0.667 | Gen: ethay airway onditioncay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioncay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab_type": "code",
        "outputId": "d0dfdc06-1adf-4157-acb3-d1312f49dbfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioncay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size(0)\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = (k @ q.transpose(1,2))/ self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = attention_weights.transpose(1,2) @ v\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size(0)\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = (k @ q.transpose(1,2))/ self.scaling_factor\n",
        "        mask = torch.tril(unnormalized_attention)\n",
        "        mask[mask == 0] = self.neg_inf\n",
        "        attention_weights = self.softmax(mask)\n",
        "        context = attention_weights.transpose(1,2) @ v\n",
        "        return context, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        # IMPORTANT CORRECTION: NON-CAUSAL ATTENTION SHOULD HAVE BEEN\n",
        "        # USED IN THE TRANSFORMER ENCODER. \n",
        "        # NEW VERSION: \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        # PREVIONS VERSION: \n",
        "        # self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "        #                             hidden_size=hidden_size, \n",
        "        #                          ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        # ------------\n",
        "        # FILL THIS IN - START\n",
        "        # ------------\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded += self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "          new_annotations, self_attention_weights = self.self_attentions[i](encoded, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "          residual_annotations = annotations + new_annotations\n",
        "          new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "          annotations = residual_annotations + new_annotations\n",
        "        # ------------\n",
        "        # FILL THIS IN - END\n",
        "        # ------------\n",
        "\n",
        "        # Transformer encoder does not have a last hidden layer. \n",
        "        return annotations, None  \n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      if self.opts.cuda:\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size \n",
        "\n",
        "        # THIS LINE WAS ADDED AS A CORRECTION. \n",
        "        embed = embed + self.positional_encodings[:seq_len]       \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN - START\n",
        "          # ------------\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts\n",
        "\n",
        "          # ------------\n",
        "          # FILL THIS IN - END\n",
        "          # ------------\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab_type": "code",
        "outputId": "d6cf0dac-34f5-4848-d54a-698df062b2d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.0005,  ## INCREASE BY AN ORDER OF MAGNITUDE\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('atmosphere', 'atmosphereway')\n",
            "('self-consequence', 'elfsay-onsequencecay')\n",
            "('extinguished', 'extinguishedway')\n",
            "('began', 'eganbay')\n",
            "('change', 'angechay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 3.870 | Val loss: 3.057 | Gen: aaaaaaaaay  y yyyyy r\n",
            "Epoch:   1 | Train loss: 2.873 | Val loss: 2.770 | Gen: aaaaaaaaay aaaaa ay yay \n",
            "Epoch:   2 | Train loss: 2.630 | Val loss: 2.563 | Gen: y ay  y y           \n",
            "Epoch:   3 | Train loss: 2.476 | Val loss: 2.470 | Gen: y ay ony y y        \n",
            "Epoch:   4 | Train loss: 2.365 | Val loss: 2.399 | Gen: oeoay ay ony y y    \n",
            "Epoch:   5 | Train loss: 2.280 | Val loss: 2.333 | Gen: oeoay ay ony y onway\n",
            "Epoch:   6 | Train loss: 2.240 | Val loss: 2.292 | Gen: oay ay onay y onttungy\n",
            "Epoch:   7 | Train loss: 2.182 | Val loss: 2.239 | Gen: oeoay ay ontntay y ontuntay\n",
            "Epoch:   8 | Train loss: 2.124 | Val loss: 2.196 | Gen: oeay aaay entententay y ontntay\n",
            "Epoch:   9 | Train loss: 2.095 | Val loss: 2.154 | Gen: oay ay entay y ongay\n",
            "Epoch:  10 | Train loss: 2.056 | Val loss: 2.119 | Gen: oay ay entay y onttay\n",
            "Epoch:  11 | Train loss: 2.011 | Val loss: 2.123 | Gen: oay aaay entntntay ilay onday\n",
            "Epoch:  12 | Train loss: 1.993 | Val loss: 2.091 | Gen: oay ay encay ilay ongay\n",
            "Epoch:  13 | Train loss: 1.948 | Val loss: 2.064 | Gen: oeay ay oncay ilay ontay\n",
            "Epoch:  14 | Train loss: 1.937 | Val loss: 2.041 | Gen: oeay ay oncay ilay ingay\n",
            "Epoch:  15 | Train loss: 1.907 | Val loss: 2.001 | Gen: oeay ay encay ilay indway\n",
            "Epoch:  16 | Train loss: 1.879 | Val loss: 2.063 | Gen: eay ay incay ililay ay\n",
            "Epoch:  17 | Train loss: 1.903 | Val loss: 1.981 | Gen: eay ay oncay ilay ondway\n",
            "Epoch:  18 | Train loss: 1.848 | Val loss: 1.957 | Gen: eay ay ongntay ilay ondway\n",
            "Epoch:  19 | Train loss: 1.822 | Val loss: 1.942 | Gen: eay ay oncay ililay ongngay\n",
            "Epoch:  20 | Train loss: 1.809 | Val loss: 1.923 | Gen: eay ay oncay ilililay ulay\n",
            "Epoch:  21 | Train loss: 1.781 | Val loss: 1.899 | Gen: eay ay oncay ilily uay\n",
            "Epoch:  22 | Train loss: 1.757 | Val loss: 1.922 | Gen: eay iay incay ilay ingrway\n",
            "Epoch:  23 | Train loss: 1.758 | Val loss: 1.894 | Gen: eay iay oncay ilililay urway\n",
            "Epoch:  24 | Train loss: 1.766 | Val loss: 1.924 | Gen: eay ay onnnntway ily ongway\n",
            "Epoch:  25 | Train loss: 1.753 | Val loss: 1.866 | Gen: eteteway ay ongngngngngntntngntn ilily ongway\n",
            "Epoch:  26 | Train loss: 1.732 | Val loss: 1.869 | Gen: eay away ongningway ililay oray\n",
            "Epoch:  27 | Train loss: 1.715 | Val loss: 1.845 | Gen: eay awaway ongngngngway illy ongay\n",
            "Epoch:  28 | Train loss: 1.692 | Val loss: 1.839 | Gen: eay ay oncay ily oray\n",
            "Epoch:  29 | Train loss: 1.688 | Val loss: 1.835 | Gen: eeay away ongngngngntay illy ongay\n",
            "Epoch:  30 | Train loss: 1.675 | Val loss: 1.821 | Gen: eay ay ongncay ily ongay\n",
            "Epoch:  31 | Train loss: 1.667 | Val loss: 1.805 | Gen: eeeway away ongngongcay ilily oingay\n",
            "Epoch:  32 | Train loss: 1.670 | Val loss: 1.795 | Gen: eay ay ongngngngntingntny ily ongray\n",
            "Epoch:  33 | Train loss: 1.651 | Val loss: 1.796 | Gen: eay away ongngngngntay ily orinay\n",
            "Epoch:  34 | Train loss: 1.627 | Val loss: 1.785 | Gen: eay awaway ongngngngngngny ily ongay\n",
            "Epoch:  35 | Train loss: 1.617 | Val loss: 1.797 | Gen: eay awaway onininingntiny ily orinay\n",
            "Epoch:  36 | Train loss: 1.631 | Val loss: 1.785 | Gen: eway ay ongngngngnticay ily oingay\n",
            "Epoch:  37 | Train loss: 1.650 | Val loss: 1.816 | Gen: etetttetettthay ay ongnayonay isay ongay\n",
            "Epoch:  38 | Train loss: 1.655 | Val loss: 1.789 | Gen: eay ay onnnngonghay isay ongray\n",
            "Epoch:  39 | Train loss: 1.620 | Val loss: 1.762 | Gen: eway away onnnncay illlay ongringray\n",
            "Epoch:  40 | Train loss: 1.594 | Val loss: 1.746 | Gen: etetetethay ay ongngonchntichndcy isay ongay\n",
            "Epoch:  41 | Train loss: 1.571 | Val loss: 1.737 | Gen: etttthay away ongngongicay isay ongay\n",
            "Epoch:  42 | Train loss: 1.556 | Val loss: 1.726 | Gen: eway away ongngongcay isay orway\n",
            "Epoch:  43 | Train loss: 1.546 | Val loss: 1.722 | Gen: eway away ongngingicay ililay ongay\n",
            "Epoch:  44 | Train loss: 1.540 | Val loss: 1.711 | Gen: eay away oncay ilily oringay\n",
            "Epoch:  45 | Train loss: 1.528 | Val loss: 1.700 | Gen: eway awaway ongncay ilily ongay\n",
            "Epoch:  46 | Train loss: 1.511 | Val loss: 1.704 | Gen: eay away ongngay ilililay oringay\n",
            "Epoch:  47 | Train loss: 1.509 | Val loss: 1.695 | Gen: ethay away ongngngingcay isililay ongringay\n",
            "Epoch:  48 | Train loss: 1.495 | Val loss: 1.687 | Gen: etetetethay away ongngay isisay onggay\n",
            "Epoch:  49 | Train loss: 1.490 | Val loss: 1.681 | Gen: eway away ongngay ilway onggay\n",
            "Epoch:  50 | Train loss: 1.474 | Val loss: 1.668 | Gen: eway away ongngngingntingngngc ilily onggay\n",
            "Epoch:  51 | Train loss: 1.465 | Val loss: 1.683 | Gen: eway away ongncay ilway oringay\n",
            "Epoch:  52 | Train loss: 1.462 | Val loss: 1.674 | Gen: eway away ongningingcay ilililay oringay\n",
            "Epoch:  53 | Train loss: 1.459 | Val loss: 1.666 | Gen: eway away ongngngingngcay isway oringay\n",
            "Epoch:  54 | Train loss: 1.446 | Val loss: 1.656 | Gen: eway away ongcongway isway oringay\n",
            "Epoch:  55 | Train loss: 1.431 | Val loss: 1.652 | Gen: eway away ongningway isililay oringingrway\n",
            "Epoch:  56 | Train loss: 1.423 | Val loss: 1.634 | Gen: eway aray ongngcay isililay ongggay\n",
            "Epoch:  57 | Train loss: 1.410 | Val loss: 1.638 | Gen: ethay away ongngcay isway oringay\n",
            "Epoch:  58 | Train loss: 1.402 | Val loss: 1.633 | Gen: eway away ongngngingcay isway oringingay\n",
            "Epoch:  59 | Train loss: 1.398 | Val loss: 1.636 | Gen: ethay away ongngcay isililay oinggay\n",
            "Epoch:  60 | Train loss: 1.391 | Val loss: 1.626 | Gen: eway away ongngngway isililay ongggingrway\n",
            "Epoch:  61 | Train loss: 1.386 | Val loss: 1.621 | Gen: ethay awarway ongngngcay isililay ongginway\n",
            "Epoch:  62 | Train loss: 1.380 | Val loss: 1.639 | Gen: eway away ongngongtingtway isiligilway oringinway\n",
            "Epoch:  63 | Train loss: 1.388 | Val loss: 1.623 | Gen: ethay ay ongcay isay oringay\n",
            "Epoch:  64 | Train loss: 1.371 | Val loss: 1.604 | Gen: eway arway ongngcay isay oringay\n",
            "Epoch:  65 | Train loss: 1.359 | Val loss: 1.594 | Gen: ethay aray ongngngngngcay isay oinggay\n",
            "Epoch:  66 | Train loss: 1.353 | Val loss: 1.600 | Gen: eway away ongngngngngngcongcay isay oringingrway\n",
            "Epoch:  67 | Train loss: 1.345 | Val loss: 1.600 | Gen: ethay ararway ongngngngngcay issilay oringingway\n",
            "Epoch:  68 | Train loss: 1.341 | Val loss: 1.581 | Gen: ethay iray ongngongngngcay isway oringingway\n",
            "Epoch:  69 | Train loss: 1.324 | Val loss: 1.575 | Gen: ethay away ongngngngtingcongcin issay oringingway\n",
            "Epoch:  70 | Train loss: 1.328 | Val loss: 1.589 | Gen: ethay araray ongcongngngngway isway oinggingway\n",
            "Epoch:  71 | Train loss: 1.323 | Val loss: 1.583 | Gen: ethay awaray ongnglay isway oringingway\n",
            "Epoch:  72 | Train loss: 1.323 | Val loss: 1.568 | Gen: ethay away ongngngway isway oringingway\n",
            "Epoch:  73 | Train loss: 1.307 | Val loss: 1.584 | Gen: ethay arway ongngcongngngway iway oringingway\n",
            "Epoch:  74 | Train loss: 1.304 | Val loss: 1.569 | Gen: ethay arway ongngingngngway issay oringinway\n",
            "Epoch:  75 | Train loss: 1.290 | Val loss: 1.578 | Gen: ethay aray ongngcongngngway isway oringingr\n",
            "Epoch:  76 | Train loss: 1.289 | Val loss: 1.582 | Gen: ethay aray ongngongngngway iway oringingray\n",
            "Epoch:  77 | Train loss: 1.286 | Val loss: 1.561 | Gen: ewhay away ongngionglay isway oringingray\n",
            "Epoch:  78 | Train loss: 1.283 | Val loss: 1.572 | Gen: ethay aray ongongongticay isway oringingray\n",
            "Epoch:  79 | Train loss: 1.291 | Val loss: 1.607 | Gen: ethay aray ongongongtingtway isway onggggray\n",
            "Epoch:  80 | Train loss: 1.289 | Val loss: 1.551 | Gen: ethay aray ongonongtingtway isay oringingray\n",
            "Epoch:  81 | Train loss: 1.280 | Val loss: 1.573 | Gen: eway aray ongcongtingcay isway oringway\n",
            "Epoch:  82 | Train loss: 1.280 | Val loss: 1.564 | Gen: ethay aray ongngongnglgtay isway oinggingway\n",
            "Epoch:  83 | Train loss: 1.278 | Val loss: 1.587 | Gen: ethay arway ongongongcay issay oringray\n",
            "Epoch:  84 | Train loss: 1.276 | Val loss: 1.560 | Gen: eway arway ongngngnghay isway oinggrway\n",
            "Epoch:  85 | Train loss: 1.256 | Val loss: 1.563 | Gen: ethay arway ongngngnghay isway oringingway\n",
            "Epoch:  86 | Train loss: 1.246 | Val loss: 1.545 | Gen: ethay arrray ongngngnglay isway oringingway\n",
            "Epoch:  87 | Train loss: 1.242 | Val loss: 1.557 | Gen: ethay aray ongngngngcay isway oringggggggr\n",
            "Epoch:  88 | Train loss: 1.244 | Val loss: 1.558 | Gen: ethay aray ongngongctingcongtin isway oingggingay\n",
            "Epoch:  89 | Train loss: 1.225 | Val loss: 1.560 | Gen: ethay aray ongngngnglay isway oringgway\n",
            "Epoch:  90 | Train loss: 1.219 | Val loss: 1.550 | Gen: ethay aray ongngongway isway oingggway\n",
            "Epoch:  91 | Train loss: 1.226 | Val loss: 1.554 | Gen: ethay arway ongnitingticay issay onggggr\n",
            "Epoch:  92 | Train loss: 1.236 | Val loss: 1.542 | Gen: ethay arway ongngngngngway isway oringingway\n",
            "Epoch:  93 | Train loss: 1.214 | Val loss: 1.539 | Gen: ethay aray ongongongticay isway oringing\n",
            "Epoch:  94 | Train loss: 1.211 | Val loss: 1.554 | Gen: ethay aray ongngongtiongway isway oringgway\n",
            "Epoch:  95 | Train loss: 1.206 | Val loss: 1.545 | Gen: ethay arway ongngngway issay oringr\n",
            "Epoch:  96 | Train loss: 1.203 | Val loss: 1.545 | Gen: ethay arway ongngngway isway oingr\n",
            "Epoch:  97 | Train loss: 1.192 | Val loss: 1.531 | Gen: ethay arway ongngngway issay oingr\n",
            "Epoch:  98 | Train loss: 1.187 | Val loss: 1.543 | Gen: ethay arway ongngingway isway oringing\n",
            "Epoch:  99 | Train loss: 1.186 | Val loss: 1.535 | Gen: ethay arway ongngngway isway oringing\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arway ongngngway isway oringing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R18s80gzC6A8",
        "colab_type": "code",
        "outputId": "1f405b47-239f-4b08-f693-9690142e8a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\teteway ay-ay ingay-inghenay inay ay-onghay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn",
        "colab_type": "text"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owstslMF-wdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}